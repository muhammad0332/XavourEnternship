{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Part 1**\n",
        "\n",
        "\n",
        "*   Load the dataset\n",
        "*   print the first few row\n",
        "\n"
      ],
      "metadata": {
        "id": "NybMQ0ZfW06r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df=pd.read_csv(\"yield_df.csv\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x72_XOH7XIBp",
        "outputId": "d4bd440e-222a-4219-ea65-dacb4e3c259f"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Unnamed: 0     Area         Item  Year  hg/ha_yield  \\\n",
            "0           0  Albania        Maize  1990        36613   \n",
            "1           1  Albania     Potatoes  1990        66667   \n",
            "2           2  Albania  Rice, paddy  1990        23333   \n",
            "3           3  Albania      Sorghum  1990        12500   \n",
            "4           4  Albania     Soybeans  1990         7000   \n",
            "\n",
            "   average_rain_fall_mm_per_year  pesticides_tonnes  avg_temp  \n",
            "0                         1485.0              121.0     16.37  \n",
            "1                         1485.0              121.0     16.37  \n",
            "2                         1485.0              121.0     16.37  \n",
            "3                         1485.0              121.0     16.37  \n",
            "4                         1485.0              121.0     16.37  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 2**\n",
        "\n",
        "\n",
        "\n",
        "*   Tokenization\n",
        "\n"
      ],
      "metadata": {
        "id": "A-LFsF3UXOOL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import gutenberg\n",
        "from nltk.tokenize import sent_tokenize,word_tokenize\n",
        "\n",
        "sample_text=gutenberg.raw(\"austen-emma.txt\")\n",
        "\n",
        "Tokenize_Sentences=sent_tokenize(sample_text)\n",
        "\n",
        "Tokenize_Words=word_tokenize(Tokenize_Sentences[0])\n",
        "\n",
        "print(\"First five sentences: \\n\")\n",
        "\n",
        "for i in range(0,5):\n",
        "  print(f\"Sentence {i+1}: \\n \",Tokenize_Sentences[i])\n",
        "\n",
        "print(\"\\n  word token of first sentence: \\n\")\n",
        "print(Tokenize_Words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Di4Qburr-tWV",
        "outputId": "4f1646f0-9d1a-4b2c-b48c-3a10f9770e5b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First five sentences: \n",
            "\n",
            "Sentence 1: \n",
            "  [Emma by Jane Austen 1816]\n",
            "\n",
            "VOLUME I\n",
            "\n",
            "CHAPTER I\n",
            "\n",
            "\n",
            "Emma Woodhouse, handsome, clever, and rich, with a comfortable home\n",
            "and happy disposition, seemed to unite some of the best blessings\n",
            "of existence; and had lived nearly twenty-one years in the world\n",
            "with very little to distress or vex her.\n",
            "Sentence 2: \n",
            "  She was the youngest of the two daughters of a most affectionate,\n",
            "indulgent father; and had, in consequence of her sister's marriage,\n",
            "been mistress of his house from a very early period.\n",
            "Sentence 3: \n",
            "  Her mother\n",
            "had died too long ago for her to have more than an indistinct\n",
            "remembrance of her caresses; and her place had been supplied\n",
            "by an excellent woman as governess, who had fallen little short\n",
            "of a mother in affection.\n",
            "Sentence 4: \n",
            "  Sixteen years had Miss Taylor been in Mr. Woodhouse's family,\n",
            "less as a governess than a friend, very fond of both daughters,\n",
            "but particularly of Emma.\n",
            "Sentence 5: \n",
            "  Between _them_ it was more the intimacy\n",
            "of sisters.\n",
            "\n",
            "  word token of first sentence: \n",
            "\n",
            "['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']', 'VOLUME', 'I', 'CHAPTER', 'I', 'Emma', 'Woodhouse', ',', 'handsome', ',', 'clever', ',', 'and', 'rich', ',', 'with', 'a', 'comfortable', 'home', 'and', 'happy', 'disposition', ',', 'seemed', 'to', 'unite', 'some', 'of', 'the', 'best', 'blessings', 'of', 'existence', ';', 'and', 'had', 'lived', 'nearly', 'twenty-one', 'years', 'in', 'the', 'world', 'with', 'very', 'little', 'to', 'distress', 'or', 'vex', 'her', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Stemming**"
      ],
      "metadata": {
        "id": "IxYoiUdIXpPh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import gutenberg\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "sample_text = gutenberg.raw('austen-emma.txt')\n",
        "sentences = sent_tokenize(sample_text)\n",
        "\n",
        "porter_stemmer = PorterStemmer()\n",
        "\n",
        "def stem_words(words):\n",
        "    return [porter_stemmer.stem(word) for word in words]\n",
        "\n",
        "words = word_tokenize(sentences[0])\n",
        "\n",
        "stemmed_words = stem_words(words)\n",
        "\n",
        "print(\"Original words in the first sentence:\")\n",
        "print(words)\n",
        "\n",
        "print(\"\\nStemmed words:\")\n",
        "print(stemmed_words)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ZgXUe1FLz71",
        "outputId": "ddf1b655-fc44-4a9c-85ad-0ab8322abf8b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original words in the first sentence:\n",
            "['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']', 'VOLUME', 'I', 'CHAPTER', 'I', 'Emma', 'Woodhouse', ',', 'handsome', ',', 'clever', ',', 'and', 'rich', ',', 'with', 'a', 'comfortable', 'home', 'and', 'happy', 'disposition', ',', 'seemed', 'to', 'unite', 'some', 'of', 'the', 'best', 'blessings', 'of', 'existence', ';', 'and', 'had', 'lived', 'nearly', 'twenty-one', 'years', 'in', 'the', 'world', 'with', 'very', 'little', 'to', 'distress', 'or', 'vex', 'her', '.']\n",
            "\n",
            "Stemmed words:\n",
            "['[', 'emma', 'by', 'jane', 'austen', '1816', ']', 'volum', 'i', 'chapter', 'i', 'emma', 'woodhous', ',', 'handsom', ',', 'clever', ',', 'and', 'rich', ',', 'with', 'a', 'comfort', 'home', 'and', 'happi', 'disposit', ',', 'seem', 'to', 'unit', 'some', 'of', 'the', 'best', 'bless', 'of', 'exist', ';', 'and', 'had', 'live', 'nearli', 'twenty-on', 'year', 'in', 'the', 'world', 'with', 'veri', 'littl', 'to', 'distress', 'or', 'vex', 'her', '.']\n",
            "\n",
            "Lemitized words:\n",
            "['[', 'emma', 'by', 'jane', 'austen', '1816', ']', 'volum', 'i', 'chapter', 'i', 'emma', 'woodhous', ',', 'handsom', ',', 'clever', ',', 'and', 'rich', ',', 'with', 'a', 'comfort', 'home', 'and', 'happi', 'disposit', ',', 'seem', 'to', 'unit', 'some', 'of', 'the', 'best', 'bless', 'of', 'exist', ';', 'and', 'had', 'live', 'nearli', 'twenty-on', 'year', 'in', 'the', 'world', 'with', 'veri', 'littl', 'to', 'distress', 'or', 'vex', 'her', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Lemmatization**"
      ],
      "metadata": {
        "id": "NALvhd2RYaIm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import gutenberg\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "# Download NLTK data (if not already downloaded)\n",
        "nltk.download('punkt')\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Load the sample text from the Gutenberg corpus\n",
        "sample_text = gutenberg.raw('austen-emma.txt')\n",
        "\n",
        "sentences = sent_tokenize(sample_text)\n",
        "\n",
        "# Initialize Porter Stemmer\n",
        "\n",
        "porter_stemmer = PorterStemmer()\n",
        "\n",
        "\n",
        "\n",
        "def stem_words(words):\n",
        "    return [porter_stemmer.stem(word) for word in words]\n",
        "\n",
        "words = word_tokenize(sentences[0])\n",
        "\n",
        "stemmed_words = stem_words(words)\n",
        "\n",
        "# Initialize WordNet Lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Lemmatize the stemmed words\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in stemmed_words]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4NdY7YzXP4dm",
        "outputId": "3227a846-bd21-4d57-991e-65855e908a0d"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Stemmed words:\n",
            "['[', 'emma', 'by', 'jane', 'austen', '1816', ']', 'volum', 'i', 'chapter', 'i', 'emma', 'woodhous', ',', 'handsom', ',', 'clever', ',', 'and', 'rich', ',', 'with', 'a', 'comfort', 'home', 'and', 'happi', 'disposit', ',', 'seem', 'to', 'unit', 'some', 'of', 'the', 'best', 'bless', 'of', 'exist', ';', 'and', 'had', 'live', 'nearli', 'twenty-on', 'year', 'in', 'the', 'world', 'with', 'veri', 'littl', 'to', 'distress', 'or', 'vex', 'her', '.']\n",
            "\n",
            "Lemmatized words:\n",
            "['[', 'emma', 'by', 'jane', 'austen', '1816', ']', 'volum', 'i', 'chapter', 'i', 'emma', 'woodhous', ',', 'handsom', ',', 'clever', ',', 'and', 'rich', ',', 'with', 'a', 'comfort', 'home', 'and', 'happi', 'disposit', ',', 'seem', 'to', 'unit', 'some', 'of', 'the', 'best', 'bless', 'of', 'exist', ';', 'and', 'had', 'live', 'nearli', 'twenty-on', 'year', 'in', 'the', 'world', 'with', 'veri', 'littl', 'to', 'distress', 'or', 'vex', 'her', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Removal of words**"
      ],
      "metadata": {
        "id": "yRmPJ4UtYlQM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import gutenberg,stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "# Download NLTK data (if not already downloaded)\n",
        "nltk.download('punkt')\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load the sample text from the Gutenberg corpus\n",
        "sample_text = gutenberg.raw('austen-emma.txt')\n",
        "\n",
        "sentences = sent_tokenize(sample_text)\n",
        "\n",
        "# Initialize Porter Stemmer\n",
        "\n",
        "porter_stemmer = PorterStemmer()\n",
        "\n",
        "\n",
        "def stem_words(words):\n",
        "    return [porter_stemmer.stem(word) for word in words]\n",
        "\n",
        "words = word_tokenize(sentences[0])\n",
        "\n",
        "stemmed_words = stem_words(words)\n",
        "\n",
        "# Initialize WordNet Lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Lemmatize the stemmed words\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in stemmed_words]\n",
        "\n",
        "# Print the lemmatized words\n",
        "print(\"\\nLemmatized words:\")\n",
        "print( lemmatized_words)\n",
        "\n",
        "# Get English stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Remove stopwords from the lemmatized words\n",
        "filtered_words = [word for word in lemmatized_words if word.lower() not in stop_words]\n",
        "\n",
        "# Print the filtered words\n",
        "print(\"\\nFiltered words:\")\n",
        "print( filtered_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9QvkD-xSuam",
        "outputId": "4b63e25d-101a-476b-893f-a979ff89b577"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Lemmatized words:\n",
            "['[', 'emma', 'by', 'jane', 'austen', '1816', ']', 'volum', 'i', 'chapter', 'i', 'emma', 'woodhous', ',', 'handsom', ',', 'clever', ',', 'and', 'rich', ',', 'with', 'a', 'comfort', 'home', 'and', 'happi', 'disposit', ',', 'seem', 'to', 'unit', 'some', 'of', 'the', 'best', 'bless', 'of', 'exist', ';', 'and', 'had', 'live', 'nearli', 'twenty-on', 'year', 'in', 'the', 'world', 'with', 'veri', 'littl', 'to', 'distress', 'or', 'vex', 'her', '.']\n",
            "\n",
            "Filtered words:\n",
            "['[', 'emma', 'jane', 'austen', '1816', ']', 'volum', 'chapter', 'emma', 'woodhous', ',', 'handsom', ',', 'clever', ',', 'rich', ',', 'comfort', 'home', 'happi', 'disposit', ',', 'seem', 'unit', 'best', 'bless', 'exist', ';', 'live', 'nearli', 'twenty-on', 'year', 'world', 'veri', 'littl', 'distress', 'vex', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**impact of each preprocessing step on the corpus:**\n",
        "\n",
        "**Tokenization**\n",
        "\n",
        "\n",
        "*   Tokenization splits the text into words and sentences, making it easier to process and analyze. It breaks down the text into meaningful units, enabling further analysis at the word or sentence level.\n",
        "\n",
        "**Stemming**\n",
        "\n",
        "\n",
        "*   Stemming reduces words to their root form by removing suffixes and prefixes. It helps in normalization and reduces variations of words to a common base form.\n",
        "\n",
        "**Lemmatization**\n",
        "\n",
        "\n",
        "\n",
        "*   Lemmatization further reduces the words to their dictionary form or lemma based on their context and part of speech. It considers the meaning of the word in the sentence.\n",
        "\n",
        "\n",
        "**Stop word Removal**\n",
        "\n",
        "\n",
        "\n",
        "*  Stop word removal eliminates common words that occur frequently in the language but may not carry significant meaning for analysis, such as \"the\", \"is\", \"are\", etc.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yBEuAFmiZFGf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "****"
      ],
      "metadata": {
        "id": "CxnCwkw3WwRe"
      }
    }
  ]
}